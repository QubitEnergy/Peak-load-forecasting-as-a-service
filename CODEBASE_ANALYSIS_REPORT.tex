\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{titlesec}

% Page geometry
\geometry{
    left=2.5cm,
    right=2.5cm,
    top=3cm,
    bottom=3cm
}

% Code listings style
\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    commentstyle=\color{gray!50},
    keywordstyle=\color{blue!70},
    stringstyle=\color{red!70},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    tabsize=4,
    showstringspaces=false
}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    citecolor=green,
    pdftitle={Peak Load Forecasting Codebase Analysis},
    pdfauthor={Development Team},
    pdfsubject={Codebase Modularization Report}
}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\small Peak Load Forecasting as a Service}
\fancyhead[R]{\small Codebase Analysis Report}
\fancyfoot[C]{\thepage}

% Section formatting
\titleformat{\section}
{\Large\bfseries}
{}
{0em}
{}[\titlerule]

\titleformat{\subsection}
{\large\bfseries}
{}
{0em}
{}

% Title
\title{\textbf{Peak Load Forecasting as a Service}\\
\large Codebase Analysis \& Modularization Report}
\author{Preliminary Research Project\\
Cooperation with IFE (Institute for Energy Technology)}
\date{2025}

\begin{document}

\maketitle

\section*{Executive Summary}

This report provides a comprehensive analysis of the peak load forecasting codebase developed as part of a preliminary research collaboration with IFE. The codebase contains multiple iterations of data collection, processing, and modeling scripts resulting from exploratory development. This document maps redundancies, identifies core functionality, and proposes a modular structure to transform the research artifacts into a maintainable, production-ready service.

\textbf{Key Findings:}
\begin{itemize}
    \item \textbf{7 distinct data collection scripts} with significant overlap (Datek/Stromme API: 2 variants; Energinet API: 5 variants)
    \item \textbf{2 duplicate preprocessing pipelines} for minute-level data
    \item \textbf{1 incomplete model implementation} with missing critical methods
    \item \textbf{Multiple analysis/visualization scripts} that should be in notebooks rather than source code
    \item \textbf{Hardcoded credentials} scattered across 8+ files (security risk)
    \item \textbf{Generated artifacts} (plots, CSVs) mixed with source code
\end{itemize}

\textbf{Recommendation:} Consolidate into a clean modular structure separating data collection, processing, modeling, and analysis layers.

\section{Project Background}

\subsection{Research Context}

This project emerged from collaboration with IFE (Institute for Energy Technology) to explore commercial opportunities in energy data analytics. The initial focus was on working with meter data from shopping malls, where understanding peak consumption patterns can provide value through:

\begin{itemize}
    \item \textbf{Demand response optimization}: Helping commercial buildings reduce peak load charges
    \item \textbf{Predictive maintenance}: Identifying abnormal consumption patterns
    \item \textbf{Energy efficiency insights}: Providing actionable recommendations based on consumption patterns
\end{itemize}

\subsection{Data Sources}

The project integrates data from multiple sources:

\begin{enumerate}
    \item \textbf{Datek Sensors (via Stromme API)}
    \begin{itemize}
        \item Real-time and historical energy consumption data
        \item Multiple meters: ``Fellesanlegg Main'', ``Fellesanlegg Pole'', ``Cinema 500''
        \item Both hourly and minute-level granularity
        \item Endpoint: \texttt{https://api.stromme.io}
    \end{itemize}
    
    \item \textbf{Energinet API}
    \begin{itemize}
        \item Building-level energy data from Norwegian energy management systems
        \item Hierarchical unit structure (buildings $\rightarrow$ subunits $\rightarrow$ meters)
        \item Multiple data types: Energy, Temperature, CO$_2$, Cost, Peak High
        \item Focus on ``Ski Storsenter'' shopping mall
        \item Endpoint: \texttt{https://www.energinet.net/api}
    \end{itemize}
    
    \item \textbf{Temperature Data (Frost API)}
    \begin{itemize}
        \item Meteorological data from Norwegian Meteorological Institute
        \item Used as exogenous feature for load forecasting
        \item Station: SN17820 (E6 Nøstvet)
    \end{itemize}
\end{enumerate}

\subsection{Core Functionality}

The system aims to:
\begin{enumerate}
    \item \textbf{Collect} energy meter data from multiple sources
    \item \textbf{Process and merge} with temperature data
    \item \textbf{Extract patterns} (time intervals, base vs. peak load)
    \item \textbf{Predict peaks} 30 minutes in advance using ML models
    \item \textbf{Provide insights} for demand response and optimization
\end{enumerate}

\section{Current Codebase Structure}

\subsection{Directory Organization}

\begin{lstlisting}[language=bash, caption={Current directory structure}]
Peak-load-forecasting-as-a-service/
├── Datek_sensor_data/          # Datek/Stromme API integration
├── Energinet/                   # Energinet API integration  
├── Model_development/           # ML models and notebooks
└── README.md                    # Minimal documentation
\end{lstlisting}

\subsection{File Inventory}

\textbf{Datek/Stromme API Scripts:}
\begin{itemize}
    \item \texttt{datek.py} -- Real-time monitoring with live plots
    \item \texttt{datek\_minute.py} -- Minute-level data collection
    \item \texttt{data\_wrangling.py} -- Initial preprocessing
    \item \texttt{data\_wrangling\_2.py} -- Duplicate preprocessing (refined)
    \item \texttt{preprocessing\_minute\_data.py} -- Minute data preprocessing
    \item \texttt{minute\_data\_EDA.py} -- Exploratory data analysis
    \item \texttt{minute\_level\_patterns.py} -- Pattern extraction
    \item \texttt{api\_test.py} -- API testing/visualization
\end{itemize}

\textbf{Energinet API Scripts:}
\begin{itemize}
    \item \texttt{energinet.py} -- Basic drilldown and plotting
    \item \texttt{energinet2.py} -- Enhanced drilldown with multiple data types
    \item \texttt{energinet3.py} -- Further iteration
    \item \texttt{energinet4.py} -- Most complete version
    \item \texttt{extract\_data.py} -- Data extraction utility
    \item \texttt{energinet\_variability.py} -- Variability analysis
    \item \texttt{energinet\_variability2.py} -- Enhanced variability analysis
    \item \texttt{top\_var.py} -- Top variability units analysis
\end{itemize}

\textbf{Model Development:}
\begin{itemize}
    \item \texttt{power\_prediction\_system.py} -- Main prediction class (incomplete)
    \item \texttt{power\_peak\_forecasting\_PoC.ipynb} -- Jupyter notebook PoC
    \item \texttt{PoC\_v2.ipynb} -- Enhanced notebook with D-PAD methodology
\end{itemize}

\section{Redundancy Analysis}

\subsection{Data Collection Layer}

\subsubsection{Datek/Stromme API Clients}

\textbf{Files Analyzed:} \texttt{datek.py}, \texttt{datek\_minute.py}

\textbf{Redundancy Level: HIGH}

Both files implement:
\begin{itemize}
    \item Identical authentication token retrieval
    \item Similar API request patterns
    \item Overlapping data fetching methods
    \item Different use cases (real-time vs. batch) but same underlying API
\end{itemize}

\textbf{Evidence:}
\begin{lstlisting}[language=Python, caption={Identical authentication code in both files}]
# Identical in both files:
headers = {
    'Authorization': 'Basic Nms2dm5hZ3JlaWVtOGJuMHRycTlvZzloNWI6MW1wOTZtaWlsbm80MWRlbWJxcHFjaTRrZzdxazFnb2ZzOW5xazB0ZnFsajduMTVtam84NQ==',
    'Content-Type': 'application/x-www-form-urlencoded'
}
\end{lstlisting}

\textbf{Recommendation:} Merge into single \texttt{StrommeClient} class with methods for both hourly and minute-level data collection.

\subsubsection{Energinet API Clients}

\textbf{Files Analyzed:} \texttt{energinet.py}, \texttt{energinet2.py}, \texttt{energinet3.py}, \texttt{energinet4.py}, \texttt{extract\_data.py}

\textbf{Redundancy Level: CRITICAL}

Five different scripts implementing:
\begin{itemize}
    \item Recursive unit drilldown navigation
    \item Energy data fetching with date intervals
    \item Similar authentication (identical bearer token)
    \item Overlapping functionality with incremental improvements
\end{itemize}

\textbf{Evolution Pattern:}
\begin{itemize}
    \item \texttt{energinet.py}: Basic drilldown + plotting
    \item \texttt{energinet2.py}: Enhanced with multiple data types
    \item \texttt{energinet3.py}: Further refinements
    \item \texttt{energinet4.py}: Most complete implementation
    \item \texttt{extract\_data.py}: Utility for specific unit extraction
\end{itemize}

\textbf{Evidence:}
\begin{lstlisting}[language=Python, caption={Repeated authentication across all files}]
# Repeated across all files:
headers = {
    "Authorization": "Bearer 78uiaschs5kobwy9p85m5w4xvks9929e",
    "Accept-Language": "no"
}
\end{lstlisting}

\textbf{Recommendation:} Keep \texttt{energinet4.py} as the reference implementation, extract reusable components, archive others.

\subsection{Data Processing Layer}

\subsubsection{Preprocessing Scripts}

\textbf{Files Analyzed:} \texttt{data\_wrangling.py}, \texttt{data\_wrangling\_2.py}, \texttt{preprocessing\_minute\_data.py}

\textbf{Redundancy Level: HIGH}

All three perform:
\begin{itemize}
    \item Column selection and renaming
    \item Timestamp conversion
    \item Data sorting
    \item CSV export
\end{itemize}

\textbf{Evidence:}
\begin{lstlisting}[language=Python, caption={Duplicate preprocessing logic}]
# data_wrangling_2.py and preprocessing_minute_data.py:
desired_cols = ["time", "a", "an", "rp", "rn", "i1", "i2", "i3", "u1", "u2", "u3", "meter_id"]
rename_map = {"time": "timestamp_utc", "a": "active_power_W", ...}
\end{lstlisting}

\textbf{Recommendation:} Consolidate into single \texttt{Preprocessor} class with configurable column mappings.

\subsubsection{Temperature Data Merging}

\textbf{Files Analyzed:} \texttt{data\_wrangling.py} (partial), temperature merge logic in notebooks

\textbf{Redundancy Level: MEDIUM}

Temperature merging logic scattered across:
\begin{itemize}
    \item Standalone scripts
    \item Notebook cells
    \item Inline processing in analysis scripts
\end{itemize}

\textbf{Recommendation:} Centralize temperature merging in dedicated \texttt{TemperatureMerger} class.

\subsection{Analysis \& Variability Computation}

\subsubsection{Variability Analysis}

\textbf{Files Analyzed:} \texttt{energinet\_variability.py}, \texttt{energinet\_variability2.py}, \texttt{top\_var.py}

\textbf{Redundancy Level: MEDIUM}

Multiple implementations of:
\begin{itemize}
    \item Rolling variability computation
    \item Coefficient of variation calculation
    \item Top units ranking
\end{itemize}

\textbf{Recommendation:} Single \texttt{VariabilityAnalyzer} utility class.

\subsection{Modeling Layer}

\subsubsection{Prediction System}

\textbf{File Analyzed:} \texttt{power\_prediction\_system.py}

\textbf{Issues Identified:}
\begin{enumerate}
    \item \textbf{Missing Method}: \texttt{prepare\_features()} called but not implemented
    \item \textbf{Syntax Error}: \texttt{ions[...]} instead of \texttt{predictions[...]} (line 450)
    \item \textbf{Mixed Concerns}: Library class mixed with example usage code
    \item \textbf{Infinite Loop}: Demo code with \texttt{while True} embedded in library file
\end{enumerate}

\textbf{Evidence:}
\begin{lstlisting}[language=Python, caption={Code issues in prediction system}]
# Line 182, 295, 430, 511: prepare_features() called but not defined
prediction_features = self.prepare_features(...)

# Line 450: Typo
ions[f'Interval {interval_idx+1}'] = {...}  # Should be 'predictions'
\end{lstlisting}

\textbf{Recommendation:}
\begin{itemize}
    \item Implement missing \texttt{prepare\_features()} method
    \item Fix syntax error
    \item Separate library code from examples
    \item Move demo code to notebook or separate script
\end{itemize}

\subsection{Security Concerns}

\textbf{Critical Issue: Hardcoded Credentials}

Credentials found in:
\begin{itemize}
    \item \texttt{datek.py}: Stromme API Basic Auth token
    \item \texttt{datek\_minute.py}: Same token
    \item \texttt{energinet.py} through \texttt{energinet4.py}: Bearer token
    \item \texttt{extract\_data.py}: Bearer token
    \item \texttt{energinet\_variability.py}: Bearer token
    \item \texttt{energinet\_variability2.py}: Bearer token
\end{itemize}

\textbf{Count:} 8+ instances of hardcoded credentials

\textbf{Risk Level: HIGH} -- Security vulnerability, credential exposure risk

\textbf{Recommendation:} Immediate migration to environment variables or secure configuration management.

\section{Artifact Pollution}

\subsection{Generated Files Mixed with Source}

\textbf{Plot Files:} 50+ PNG files scattered across source directories
\begin{itemize}
    \item \texttt{Datek\_sensor\_data/}: 15+ plot files
    \item \texttt{Datek\_sensor\_data/Results\_minute\_data\_EDA/}: 13 plot files
    \item \texttt{Datek\_sensor\_data/sensor\_analysis/}: Multiple subdirectories with plots
    \item \texttt{Datek\_sensor\_data/minute\_level\_pattern\_plots/}: 34 plot files
\end{itemize}

\textbf{Data Files:} Temporary/processed CSVs in source directories
\begin{itemize}
    \item \texttt{all\_minute\_data.csv}
    \item \texttt{processed\_minute\_data\_datek\_API.csv}
    \item \texttt{energy\_and\_temperature.csv}
    \item Multiple Energinet output CSVs
\end{itemize}

\textbf{Recommendation:} Move all generated artifacts to \texttt{data/outputs/} and \texttt{data/processed/}.

\subsection{Documentation Artifacts}

\textbf{PDFs:} Research papers and documentation in code directories
\begin{itemize}
    \item \texttt{D-PAD paper.pdf}
    \item \texttt{Day-Ahead\_Electricity\_Consumption\_Prediction...pdf}
    \item \texttt{EnerginettDataStruktur.pdf}
    \item \texttt{datauthentingApi2022.pdf}
    \item \texttt{EDA.pdf}
\end{itemize}

\textbf{Recommendation:} Move to \texttt{docs/} or \texttt{references/} directory.

\section{Proposed Modular Structure}

\subsection{Recommended Organization}

\begin{lstlisting}[language=bash, caption={Proposed modular structure}]
peak_load_forecasting/
├── src/
│   ├── data_collectors/
│   │   ├── __init__.py
│   │   ├── stromme_client.py      # Consolidated Datek/Stromme client
│   │   └── energinet_client.py    # Consolidated Energinet client
│   ├── data_processors/
│   │   ├── __init__.py
│   │   ├── preprocessor.py        # Unified preprocessing
│   │   └── temperature_merger.py  # Temperature data merging
│   ├── models/
│   │   ├── __init__.py
│   │   └── peak_predictor.py      # Complete prediction system
│   └── utils/
│       ├── __init__.py
│       ├── config.py              # Configuration management
│       └── variability.py         # Variability analysis
├── config/
│   ├── config.example.yaml        # Template for credentials
│   └── meters.yaml                # Meter configurations
├── data/
│   ├── raw/                       # Original data files
│   ├── processed/                 # Cleaned/processed data
│   └── outputs/                   # Generated plots, reports
├── notebooks/
│   ├── exploration/               # EDA notebooks
│   └── poc/                       # PoC notebooks
├── tests/
│   ├── test_collectors.py
│   ├── test_processors.py
│   └── test_models.py
├── docs/
│   └── references/                # PDFs, papers
├── requirements.txt
└── README.md
\end{lstlisting}

\subsection{Rationale for Organization}

\paragraph{Separation of Concerns}
\begin{itemize}
    \item \textbf{Data Collectors}: Isolated API clients, easy to test and replace
    \item \textbf{Data Processors}: Reusable preprocessing pipelines
    \item \textbf{Models}: Clean ML implementation separate from data access
    \item \textbf{Utils}: Shared utilities without business logic dependencies
\end{itemize}

\paragraph{Security \& Configuration}
\begin{itemize}
    \item Centralized credential management in \texttt{config/}
    \item Environment-based configuration (dev/test/prod)
    \item No hardcoded secrets in source code
\end{itemize}

\paragraph{Artifact Management}
\begin{itemize}
    \item Clear separation: source code vs. generated files
    \item Data pipeline: \texttt{raw} $\rightarrow$ \texttt{processed} $\rightarrow$ \texttt{outputs}
    \item Notebooks isolated from production code
\end{itemize}

\paragraph{Maintainability}
\begin{itemize}
    \item Single responsibility per module
    \item Easy to locate functionality
    \item Clear dependencies between layers
\end{itemize}

\paragraph{Testability}
\begin{itemize}
    \item Each layer independently testable
    \item Mock external API calls
    \item Isolated business logic
\end{itemize}

\section{Files to Keep vs. Remove}

\subsection{Files to Consolidate and Keep}

\textbf{Data Collection:}
\begin{itemize}
    \item Keep: \texttt{datek.py} + \texttt{datek\_minute.py} $\rightarrow$ Merge into \texttt{stromme\_client.py}
    \item Keep: \texttt{energinet4.py} $\rightarrow$ Refactor into \texttt{energinet\_client.py}
    \item Keep: \texttt{extract\_data.py} $\rightarrow$ Incorporate utility functions into client
\end{itemize}

\textbf{Data Processing:}
\begin{itemize}
    \item Keep: \texttt{preprocessing\_minute\_data.py} or \texttt{data\_wrangling\_2.py} $\rightarrow$ Merge into \texttt{preprocessor.py}
    \item Keep: Temperature merge logic $\rightarrow$ Extract to \texttt{temperature\_merger.py}
\end{itemize}

\textbf{Models:}
\begin{itemize}
    \item Keep: \texttt{power\_prediction\_system.py} $\rightarrow$ Fix and refactor into \texttt{peak\_predictor.py}
\end{itemize}

\textbf{Analysis:}
\begin{itemize}
    \item Keep: \texttt{energinet\_variability2.py} $\rightarrow$ Extract to \texttt{variability.py} utility
\end{itemize}

\subsection{Files to Archive/Remove}

\textbf{Obsolete Iterations:}
\begin{itemize}
    \item Remove: \texttt{energinet.py}, \texttt{energinet2.py}, \texttt{energinet3.py} (superseded by \texttt{energinet4.py})
    \item Remove: \texttt{data\_wrangling.py} (superseded by \texttt{data\_wrangling\_2.py})
    \item Remove: \texttt{energinet\_variability.py} (superseded by \texttt{energinet\_variability2.py})
\end{itemize}

\textbf{One-off Scripts:}
\begin{itemize}
    \item Move to notebooks: \texttt{api\_test.py}, \texttt{minute\_data\_EDA.py}, \texttt{top\_var.py}
    \item Move to notebooks: \texttt{minute\_level\_patterns.py}
\end{itemize}

\textbf{Configuration/Reference:}
\begin{itemize}
    \item Move to \texttt{docs/references/}: All PDF files
    \item Move to \texttt{config/}: \texttt{units.csv} (with sanitization)
\end{itemize}

\subsection{Generated Artifacts to Reorganize}

\begin{itemize}
    \item Move all \texttt{.png} files to \texttt{data/outputs/plots/}
    \item Move processed CSVs to \texttt{data/processed/}
    \item Move raw CSVs to \texttt{data/raw/} (if not regeneratable)
\end{itemize}

\section{Critical Issues Requiring Immediate Attention}

\subsection{Code Issues}

\begin{enumerate}
    \item \textbf{Missing Implementation} (Blocking)
    \begin{itemize}
        \item \texttt{prepare\_features()} method not implemented in \texttt{PowerPeakPredictor}
        \item \textbf{Impact}: Training and prediction pipeline will fail
        \item \textbf{Fix Required}: Implement feature engineering logic
    \end{itemize}
    
    \item \textbf{Syntax Error} (Blocking)
    \begin{itemize}
        \item Line 450: \texttt{ions[...]} should be \texttt{predictions[...]}
        \item \textbf{Impact}: Runtime error in fallback prediction path
        \item \textbf{Fix Required}: Correct variable name
    \end{itemize}
    
    \item \textbf{Mixed Concerns} (Code Quality)
    \begin{itemize}
        \item Library class contains example usage code
        \item \textbf{Impact}: Difficult to import and use as library
        \item \textbf{Fix Required}: Separate library from examples
    \end{itemize}
\end{enumerate}

\subsection{Security Issues}

\begin{enumerate}
    \item \textbf{Hardcoded Credentials} (Critical)
    \begin{itemize}
        \item 8+ instances across codebase
        \item \textbf{Impact}: Security vulnerability, credential exposure
        \item \textbf{Fix Required}: Migrate to environment variables immediately
    \end{itemize}
    
    \item \textbf{Token in Version Control} (Critical)
    \begin{itemize}
        \item Credentials may be in git history
        \item \textbf{Impact}: Permanent security risk
        \item \textbf{Fix Required}: Rotate all tokens, implement \texttt{.gitignore} for secrets
    \end{itemize}
\end{enumerate}

\subsection{Architecture Issues}

\begin{enumerate}
    \item \textbf{No Configuration Management}
    \begin{itemize}
        \item Settings scattered throughout code
        \item \textbf{Impact}: Difficult to manage different environments
        \item \textbf{Fix Required}: Centralized configuration system
    \end{itemize}
    
    \item \textbf{Tight Coupling}
    \begin{itemize}
        \item API clients, processors, and models mixed together
        \item \textbf{Impact}: Difficult to test and maintain
        \item \textbf{Fix Required}: Modular separation
    \end{itemize}
\end{enumerate}

\section{Recommendations Summary}

\subsection{Immediate Actions (Priority 1)}

\begin{enumerate}
    \item \textbf{Fix Critical Bugs}
    \begin{itemize}
        \item Implement \texttt{prepare\_features()} method
        \item Fix \texttt{ions} $\rightarrow$ \texttt{predictions} typo
        \item Verify prediction pipeline works end-to-end
    \end{itemize}
    
    \item \textbf{Security Remediation}
    \begin{itemize}
        \item Extract all credentials to environment variables
        \item Rotate API tokens (assume exposure)
        \item Add \texttt{.env} to \texttt{.gitignore}
        \item Create \texttt{config.example.yaml} template
    \end{itemize}
    
    \item \textbf{Document Core Functionality}
    \begin{itemize}
        \item Document data collection APIs
        \item Document prediction model interface
        \item Create usage examples
    \end{itemize}
\end{enumerate}

\subsection{Short-term Refactoring (Priority 2)}

\begin{enumerate}
    \item \textbf{Consolidate Data Collectors}
    \begin{itemize}
        \item Merge Datek scripts into single client
        \item Merge Energinet scripts (keep best version)
        \item Extract common authentication logic
    \end{itemize}
    
    \item \textbf{Unify Preprocessing}
    \begin{itemize}
        \item Single preprocessing pipeline
        \item Configurable column mappings
        \item Temperature merge as separate utility
    \end{itemize}
    
    \item \textbf{Separate Library from Examples}
    \begin{itemize}
        \item Clean library interfaces
        \item Move example code to notebooks
        \item Create simple CLI or example scripts
    \end{itemize}
\end{enumerate}

\subsection{Medium-term Improvements (Priority 3)}

\begin{enumerate}
    \item \textbf{Implement Modular Structure}
    \begin{itemize}
        \item Create \texttt{src/} package structure
        \item Separate layers (collectors, processors, models)
        \item Add \texttt{\_\_init\_\_.py} files and proper imports
    \end{itemize}
    
    \item \textbf{Artifact Organization}
    \begin{itemize}
        \item Move generated files to \texttt{data/outputs/}
        \item Move references to \texttt{docs/}
        \item Clean up source directories
    \end{itemize}
    
    \item \textbf{Testing Infrastructure}
    \begin{itemize}
        \item Unit tests for collectors
        \item Unit tests for processors
        \item Integration tests for prediction pipeline
    \end{itemize}
    
    \item \textbf{Documentation}
    \begin{itemize}
        \item API documentation
        \item Installation guide
        \item Usage examples
        \item Architecture overview
    \end{itemize}
\end{enumerate}

\section{Conclusion}

This codebase represents valuable exploratory work in peak load forecasting for commercial buildings. The iterative development process, while productive for research, has resulted in significant redundancy and technical debt. The analysis reveals:

\begin{itemize}
    \item \textbf{7 redundant data collection scripts} that should be 2 unified clients
    \item \textbf{3 preprocessing pipelines} that should be 1 configurable processor
    \item \textbf{Critical bugs} preventing the model from functioning
    \item \textbf{Security vulnerabilities} from hardcoded credentials
    \item \textbf{Mixed concerns} making the codebase difficult to maintain
\end{itemize}

The proposed modular structure addresses these issues by:

\begin{itemize}
    \item \textbf{Separating concerns} into clear layers
    \item \textbf{Consolidating functionality} into reusable components
    \item \textbf{Improving security} through proper configuration management
    \item \textbf{Enabling testing} through isolated, testable modules
    \item \textbf{Facilitating maintenance} through clear organization
\end{itemize}

With these improvements, the codebase can transition from a research prototype to a maintainable, production-ready service that supports the startup opportunity exploration with IFE.

\appendix

\section{Detailed File Mapping}

\begin{longtable}{p{4cm} p{2cm} p{4.5cm} p{4cm}}
\toprule
\textbf{Current File} & \textbf{Status} & \textbf{Destination} & \textbf{Notes} \\
\midrule
\endfirsthead

\toprule
\textbf{Current File} & \textbf{Status} & \textbf{Destination} & \textbf{Notes} \\
\midrule
\endhead

\midrule
\multicolumn{4}{r}{{Continued on next page}} \\
\midrule
\endfoot

\bottomrule
\endlastfoot

\texttt{Datek\_sensor\_data/datek.py} & Merge & \texttt{src/data\_collectors/stromme\_client.py} & Combine with datek\_minute.py \\
\texttt{Datek\_sensor\_data/datek\_minute.py} & Merge & \texttt{src/data\_collectors/stromme\_client.py} & Combine with datek.py \\
\texttt{Datek\_sensor\_data/data\_wrangling.py} & Remove & -- & Superseded by data\_wrangling\_2.py \\
\texttt{Datek\_sensor\_data/data\_wrangling\_2.py} & Keep & \texttt{src/data\_processors/preprocessor.py} & Refactor into class \\
\texttt{Datek\_sensor\_data/preprocessing\_minute\_data.py} & Merge & \texttt{src/data\_processors/preprocessor.py} & Merge with data\_wrangling\_2.py \\
\texttt{Datek\_sensor\_data/api\_test.py} & Move & \texttt{notebooks/exploration/api\_test.ipynb} & Convert to notebook \\
\texttt{Datek\_sensor\_data/minute\_data\_EDA.py} & Move & \texttt{notebooks/exploration/} & Already exploratory \\
\texttt{Datek\_sensor\_data/minute\_level\_patterns.py} & Move & \texttt{notebooks/exploration/} & Analysis script \\
\texttt{Energinet/energinet.py} & Remove & -- & Superseded by energinet4.py \\
\texttt{Energinet/energinet2.py} & Remove & -- & Superseded by energinet4.py \\
\texttt{Energinet/energinet3.py} & Remove & -- & Superseded by energinet4.py \\
\texttt{Energinet/energinet4.py} & Keep & \texttt{src/data\_collectors/energinet\_client.py} & Refactor into class \\
\texttt{Energinet/extract\_data.py} & Merge & \texttt{src/data\_collectors/energinet\_client.py} & Incorporate utilities \\
\texttt{Energinet/energinet\_variability.py} & Remove & -- & Superseded by variability2 \\
\texttt{Energinet/energinet\_variability2.py} & Keep & \texttt{src/utils/variability.py} & Extract analysis functions \\
\texttt{Energinet/top\_var.py} & Move & \texttt{notebooks/exploration/} & Analysis script \\
\texttt{Model\_development/power\_prediction\_system.py} & Fix \& Keep & \texttt{src/models/peak\_predictor.py} & Fix bugs, refactor \\
\texttt{Model\_development/power\_peak\_forecasting\_PoC.ipynb} & Keep & \texttt{notebooks/poc/} & Research artifact \\
\texttt{Model\_development/PoC\_v2.ipynb} & Keep & \texttt{notebooks/poc/} & Research artifact \\
\texttt{Energinet/units.csv} & Keep & \texttt{config/meters.yaml} & Convert to YAML, sanitize \\
\end{longtable}

\section{Credential Locations}

All instances requiring credential extraction:

\begin{enumerate}
    \item \texttt{Datek\_sensor\_data/datek.py} -- Line 45
    \item \texttt{Datek\_sensor\_data/datek\_minute.py} -- Line 22
    \item \texttt{Energinet/energinet.py} -- Line 12
    \item \texttt{Energinet/energinet2.py} -- Line 12
    \item \texttt{Energinet/energinet3.py} -- Line 12
    \item \texttt{Energinet/energinet4.py} -- Line 12
    \item \texttt{Energinet/extract\_data.py} -- Line 12
    \item \texttt{Energinet/energinet\_variability.py} -- Line 13
    \item \texttt{Energinet/energinet\_variability2.py} -- Line 13
\end{enumerate}

\textbf{Action Required:} Create \texttt{config/config.example.yaml} with placeholder values and migration guide.

\vspace{1cm}
\hrule
\vspace{0.5cm}

\noindent\textit{Report generated: 2025}\\

\noindent\textit{For questions or clarifications, refer to the codebase or contact the development team.}

\end{document}

